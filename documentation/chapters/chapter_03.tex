\textcite{nguyen2013lexical} have studied SMT models to migrate source code written in one programming language into another. They used Phrasal, a phrase-based SMT model, to perform method-level translation from Java to C\#. For training Phrasal, they treated each method and each code token from two open-source projects, db4o and Lucene, as a sentence and a word, respectively. The results show that SMT is a viable approach for performing source code migration.
\\\\
To adapt codes written in Python 2 to Python 3, \textcite{aggarwal2015using} employed a SMT approach. For this purpose, the authors used a tool called 2to3 to convert Python 2 projects—Django and Natural Language Toolkit—to Python 3; they used Giza++ for aligning codes and Moses for constructing the translation model. The high BLEU scores achieved in their experiment demonstrate that traditional SMT models can be used to translate programs written in Python 2 to Python 3 with some postprocessing. And according to the authors, programming languages can be viewed as natural languages.
\\\\
\textcite{lachaux2020unsupervised} have proposed an unsupervised neural transpiler called TransCoder. The TransCoder is an encoder-decoder model based on the transformer architecture. To build this model, they constructed monolingual datasets using the functions extracted from C++, Java, and Python open source projects available on GitHub. The authors trained the model using the concept of unsupervised neural machine translation. First, they performed cross-lingual programming language model pretraining with a masked language modeling objective. Second, they trained the model with a Denoising Auto-Encoding (DAE) objective to encode and decode data sequences. Finally, they used a back-translation approach to increase the translation quality. The authors built the TransCoder model with a transformer having 6 layers, 8 attention heads, and 1024 dimensions, and used beam search decoding for inference. They experimented with the model by translating the source codes from GeeksforGeeks and evaluating it using computational accuracy, reference match, and BLEU score. With TransCoder, the authors demonstrated that the model can be used for any programming language without any specific knowledge.
\newpage
In \cite{hassan2020neural},  \citeauthor{hassan2020neural} have built a transformer-based code converter to translate Java codes to Swift and Swift codes to Java. Due to the unavailability of the Java-Swift parallel dataset, the authors generated synthetic data, which consists of random independent codes with arbitrary variable names and literals. They trained the transformer model using Open-NMT. The model had a token accuracy ranging from 70\% to 90\%.
\\\\
\textcite{rothe2020leveraging} have experimented with BERT, Generative Pretrained Transformer (GPT) 2, and RoBERTa models. They used the pretrained version of these models to initialize a number of sequence-to-sequence models. The different combination of models were then tested on several tasks such as summarization, translation, etc. The evaluation of the different combinations of the models shows that BERT2RND, which is BERT with a randomly initialized decoder, and BERTShare, were the best performing models in terms of translation.
\\\\
\textcite{ahmad2021avatar} have presented a Java-Python parallel dataset constructed from the solutions of 8,475 programming problems. The authors trained the NMT models from scratch as well as fine-tuned a few pretrained models, viz. CodeGPT, CodeBERT, GraphCodeBERT, and Program and Language Bidirectional and Auto-Regressive Transformers (PLBART). The experimental results show that the translation models did perform well in terms of lexical matching. However, the models were not able to produce syntactically correct codes with significant data-flow matching. Based on the results of the performance evaluation, PLBART was the best performing model with a 67.1 BLEU score and a 43.3 CodeBLEU score. In terms of BLEU score, the transformer and CodeBERT models did well.
\\\\
\textcite{tauda2021programming} have designed a translator for server-side and client-side programming languages which is based on the Long Short-Term Memory model. The authors trained the model twice using the 102 pairs of data types and decorators of Typescript and the Kotlin language. The model was built to translate applications written using the NestJS framework to client-based Android applications.
\\\\
In \cite{daiattempts}, \citeauthor{daiattempts} have introduced an approach for improving the quality of programming language translation. In this approach, the authors interpreted source code in terms of the syntax tree, as the syntax tree contains a lot of information about the structure of code. The authors compared this syntax tree representation of code to the text representation of code in program translation from Java to Python and vice-versa. They trained the transformer-based translation model using an unsupervised neural machine translation approach. The translation results show that the translations have a very low BLEU score. However, the model trained with their syntax tree-based approach was able to generate important structures like conditional statements and loops when compared to the model trained with the text representation of code.
\\\\
In \cite{shah2021natural}, \citeauthor{shah2021natural} have attempted to translate natural language to the Python programming language. They trained the transformer model using the Django dataset. The translation model was able to learn the Python syntax, but it had difficulty understanding variable names.
\\\\
To fix a defective Java program, \textcite{mashhadi2021applying} suggested an automated CodeBERT-based approach. The authors used an encoder-decoder model to construct the program repair model. On the encoder part, they used CodeBERT, and on the decoder part, the authors used a transformer decoder. The accuracy of the model shows that this approach is a feasible solution to fix bugs.
\\\\
To study whether the representations yielded from the pretrained models contain the attributes of the source code, \textcite{karmakar2021pre} have introduced four tasks. These tasks determine whether the code representations obtained from the pretrained models contain surface-level, syntactic, structural, and semantic information. The tasks include checking if the model can generate a valid token, predicting code complexity, predicting code length, and detecting invalid types. For evaluation, the authors trained a classifier which takes as input the hidden layer of a pretrained model. The probe analyses showed that CodeBERT delivered promising results in terms of syntactic and semantic understanding.
\\\\
In \cite{zhu2022multilingual}, \citeauthor{zhu2022multilingual} have presented a parallel dataset containing data from 7 different programming languages. The authors used a transformer model with 12 encoder layers, 6 decoder layers, 768 dimensions, and 12 attention heads. They initialized the model with the pretrained weights. To understand the similarities between different languages, the authors first trained the model with the DAE objective on a snippets dataset. Next, the model was pretrained for multilingual snippet translation to improve the quality of program translation. The authors used code snippets for two reasons: snippets are shorter in length than programs, and snippets help to learn long-distance dependencies. The BLEU and CodeBLEU scores show that DAE and multilingual snippet pretraining improved the translation performance of the model.

