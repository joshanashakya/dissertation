@inbook{sommerville_2002,
  author = {Sommerville, Ian},
  title = {Source Code Translation},
  booktitle = {Software Engineering},
  year = {2002},
  bookauthor = {Sommerville, Ian},
  edition = {6},
  publisher = {Addison Wesley},
  chapter = {28},
  pages = {573--574},
  url = {https://ifs.host.cs.st-andrews.ac.uk/Resources/Notes/Evolution/SWReeng.pdf}
} 

@article{lachaux2020unsupervised,
  title={Unsupervised translation of programming languages},
  author={Roziere, Baptiste and Lachaux, Marie-Anne and Chanussot, Lowik and Lample, Guillaume},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20601--20611},
  year={2020}
}

@article{ahmad2021avatar,
  author={Ahmad, Wasi Uddin and Tushar, Md Golam Rahman and Chakraborty, Saikat and Chang, Kai-Wei},
  title={AVATAR: A parallel corpus for java-python program translation},
  journal={arXiv preprint arXiv:2108.11590},
  year={2021}
}

@unpublished{zhu2022multilingual,
  author={Zhu, Ming-Yuan and Suresh, Karthik and Reddy, Chandan K},
  title={Multilingual Code Snippets Training for Program Translation},
  journal={AAAI Conference on Artificial Intelligence 2022},
  organization={Association for the Advancement of Artificial Intelligence},
  note={unpublished}
}

@misc{taverna_2022,
  author={Taverna, Maurizio}, 
  title={Language2Language transformers: Machine learning to build transpilers},
  organization={tomassetti.me}, 
  url={https://tomassetti.me/language2language-transformers-machine-learning-to-build-transpilers/},
  urldate = {2022-05-14}
} 

@misc{the_big_language_team_2022,
  author = {Big Language Solutions},
  title = {Everything You Need to Know about Neural Machine Translation},
  url = {https://biglanguage.com/blog/everything-to-know-about-neural-machine-translation},  
  organization={biglanguage.com},
  urldate = {2022-05-14}
}

@misc{jennifer_villa2018Oct,
  author = {Jennifer Villa and Yoav Zimmerman},
  title = {Warm starting for efficient deep learning resource utilization},
  url = {https://www.determined.ai/blog/warm-starting-deep-learning},
  organization = {determined.ai},
  urldate = {2022-05-14}
}

@article{aggarwal2015using,
  author={Aggarwal, Karan and Salameh, Mohammad and Hindle, Abram},
  title={Using machine translation for converting python 2 to python 3 code},
  journal={PeerJ PrePrints},
  year={2015}
}

@article{chen2018tree,
  title={Tree-to-tree neural networks for program translation},
  author={Chen, Xinyun and Liu, Chang and Song, Dawn},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@misc{omniscien_technologies_2020,
  author = {Omniscien Technologies},
  title = {When, Why and How to Migrate from Statistical to Neural Machine},
  organization = {omniscien.com},
  url = {https://omniscien.com/blog/migrate-from-smt-to-nmt},
  urldate = {2022-05-22}
}

@article{vaswani2017attention,
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  title={Attention is all you need},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@book{rothman2021transformers,
  title={Transformers for Natural Language Processing: Build innovative deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT, RoBERTa, and more},
  author={Rothman, Denis},
  year={2021},
  publisher={Packt Publishing Ltd}
}

@misc{alammar, 
  author={Alammar, Jay},
  title={The illustrated transformer}, 
  url={https://jalammar.github.io/illustrated-transformer/},
  organization={jalammar.github.io},
  urldate = {2022-05-22}
} 

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@misc{Durgia2022Jan,
  author = {Durgia, Chandan},
  title = {Exploring BERT variants (Part 1): ALBERT, RoBERTa, ELECTRA},
  organization = {towardsdatascience.com},
  url = {https://towardsdatascience.com/exploring-bert-variants-albert-roberta-electra-642dfe51bc23},
  urldate = {2022-05-22}
}

@article{feng2020codebert,
  title={Codebert: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
}

@misc{AnalyticsIndiaMagazine2020,
  author = {Analytics India Magazine},
  title = {{Microsoft Introduces First Bimodal Pre-Trained Model for Natural Language Generation}},
  organization = {analyticsindiamag.com},
  url = {https://analyticsindiamag.com/microsoft-introduces-first-bimodal-pre-trained-model-for-natural-language-generation},
  urldate = {2022-05-22}
}

@inproceedings{hassan2020neural,
  title={Neural Machine Based Mobile Applications Code Translation},
  author={Hassan, Mohammed H and Mahmoud, Omar A and Mohammed, Omar I and Baraka, Ammar Y and Mahmoud, Amira T and Yousef, Ahmed H},
  booktitle={2020 2nd Novel Intelligent and Leading Emerging Sciences Conference (NILES)},
  pages={302--307},
  year={2020},
  organization={IEEE}
}

@inproceedings{tauda2021programming,
  title={Programming Language Translator For Integration Client Application With Web APIs},
  author={Tauda, Mudiarta and Zainuddin, Zahir and Tahir, Zulkifli},
  booktitle={2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS)},
  pages={1--4},
  year={2021},
  organization={IEEE}
}

@report{daiattempts,
  author={Dai Feng and Shigeru Chiba},
  title={Attempts on using syntax trees to improve programming language translation quality by machine learning},
  institution="Graduate School of Information Science and Technology, The University of Tokyo",
  year={2021},
  type={Technical Report},
  url={https://static.csg.ci.i.u-tokyo.ac.jp/papers/21/daifeng-jssst2021.pdf}
}

@inproceedings{nguyen2013lexical,
  title={Lexical statistical machine translation for language migration},
  author={Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Tien N},
  booktitle={Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
  pages={651--654},
  year={2013}
}

@inproceedings{karmakar2021pre,
  title={What do pre-trained code models know about code?},
  author={Karmakar, Anjan and Robbes, Romain},
  booktitle={2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={1332--1336},
  year={2021},
  organization={IEEE}
}

@inproceedings{shah2021natural,
  title={Natural Language to Python Source Code using Transformers},
  author={Shah, Meet and Shenoy, Rajat and Shankarmani, Radha},
  booktitle={2021 International Conference on Intelligent Technologies (CONIT)},
  pages={1--4},
  year={2021},
  organization={IEEE}
}

@inproceedings{mashhadi2021applying,
  title={Applying codebert for automated program repair of java simple bugs},
  author={Mashhadi, Ehsan and Hemmati, Hadi},
  booktitle={2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)},
  pages={505--509},
  year={2021},
  organization={IEEE}
}

@article{rothe2020leveraging,
  title={Leveraging pre-trained checkpoints for sequence generation tasks},
  author={Rothe, Sascha and Narayan, Shashi and Severyn, Aliaksei},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={264--280},
  year={2020},
  publisher={MIT Press}
}

@misc{manning2019cs224n,
  author={Manning, Christopher and Socher, Richard and Fang, Guillaume Genthial and Mundra, Rohit},
  title={CS224n: Natural Language Processing with Deep Learning},
  organization={web.stanford.edu},
  url = {https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194}
}

@article{ren2020codebleu,
  title={Codebleu: a method for automatic evaluation of code synthesis},
  author={Ren, Shuo and Guo, Daya and Lu, Shuai and Zhou, Long and Liu, Shujie and Tang, Duyu and Sundaresan, Neel and Zhou, Ming and Blanco, Ambrosio and Ma, Shuai},
  journal={arXiv preprint arXiv:2009.10297},
  year={2020}
}

@misc{jurafsky_martin, 
  title={Basic text processing}, 
  url={https://web.stanford.edu/~jurafsky/slp3/slides/2_TextProc_Jan_06_2021.pdf}, 
  author={Jurafsky, Dan and Martin, James H.},
  organization={web.stanford.edu},
  urldate={2022-06-20}
} 

@misc{kazemnejad_2019,
  title={How to do deep learning research with absolutely no gpus - part 2}, 
  url={https://kazemnejad.com/blog/how_to_do_deep_learning_research_with_absolutely_no_gpus_part_2/}, 
  organization={kazemnejad.com}, 
  author={Kazemnejad, Amirhossein}, 
  urldate={2022-06-20}
} 

@misc{hansen_2019, 
  title={Activation functions explained - Gelu, Selu, Elu, relu and more}, 
  url={https://mlfromscratch.com/activation-functions-explained/}, journal={Machine Learning From Scratch}, 
  organization={mlfromscratch.com}, 
  author={Hansen, Casper}, 
  urldate={2022-06-25}
} 

@misc{ng, 
  title={Optimization Algorithms}, 
  url={https://cs230.stanford.edu/files/C2M2.pdf}, 
  organization={DeepLearning.AI}, 
  author={Ng, Andrew},
  urldate={2022-06-25}
} 
